# 1. 模型表达（Model Representation）

线性回归是机器学习的最基本的算法，线性回归是为了拟合变量之间的线性关系。开始之前先将变量含义定义清楚：

$$x_{j}^{(i)}$$: 样本i中j特征的值

$$x^{(i)}: 样本i所有特征值$$

$$m: 样本总数$$

$$n: 特征总数$$

线性回归的函数模型如下：


$$
h_{\theta }(x)=\theta _{0}+\theta _{1}x_{1}+\theta _{2}x_{2}+\theta _{3}x_{3}+ ... + \theta _{n}x_{n}
$$


精简表达式，用向量表达为：


$$
h_{\theta }(x)= [\theta_{0} \quad      \theta_{1} \quad...\quad \theta_{n}]\left [\begin{matrix} x_{0}\\ x_{1}\\ ...\\ x_{n}\\ \end{matrix} \right]  = \theta ^{T}x
$$


注：对上述表达式中的其中x\_{0}^{\(i\)}=0,  其中\(i\in 1,...m\)

# 2. 损失函数\(Cost Function\)

损失函数表示的是模型预测的值与训练集中实际值的差距，我们的目标就是找出使得建模误差的平方和能够最小的模型参数。


$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^{2}
$$


# 3. 梯度下降\(Gradient Descent\)

梯度下降就是一个用来求函数最小值的算法，梯度下降背后的思想是：开始时我们随机选择一个参数的组合\($$\theta_{0},\theta_{1},...,\theta_{n}$$\)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（local minimum）,因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum）,选择不同的初始参数组合，可能会找到不同的局部最小值。

批量梯度下降（batch gradient descent）算法的公式为：

Repeat until convergence {


$$
\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial \theta_{j}}J(\theta)
$$


}

Repeat {


$$
 \theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{i}) - y^{(i)})x_{j}^{(i)}
$$


}              \( 每次同时更新所有\theta\_j,  j=0,...n\)

其中\alpha是学习率（learning rate）,它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。

# 4.特征缩放\(Features Scaling\)

在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。最简单的方法是令：


$$
x_{n}=\frac{x_{n}-\mu_{n}}{s_{n}}
$$


其中, 其中\mu_{n}是平均值，是平均值，s_{n}$$是标准差

# 5.学习率\(Learning Rate\)

梯度下降算法的每次迭代受到学习率的影响，如果学习率α过小，则达到收敛所需的迭代次数会非常高；如果学习率α过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

通常可以考虑尝试这些学习率：$$\alpha=0.01,\ 0.3,\ 0.1,\ 0.3,\ 1,\ 3,\ 10$$

# 6.多项式回归\(Polynomial Regression\)

线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：


$$
h_\theta(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^{2}
$$


或者三次方模型：


$$
h_\theta(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^{2}+\theta_{3}x_{3}^{3}
$$


![](/assets/Polynomial Regression.jpg)

# 7.正规方程

到目前为止，求解损失函数最小值都是通过梯度下降算法，对于某些线性回复问题，正规方程是更好的解决方案。

假设我们的训练集特征矩阵为$$X$$（包含了$$x_{0}=1$$）并且我们的训练集结果为向量$$y$$,则利用正规方程解出向量

$$\theta=(X^{T}X)^{-1}X^{T}y$$， $$T$$表示矩阵转置，$$-1$$表示矩阵的逆

梯度下降与正规方程的比较：

| 梯度下降 | 正规方程 |
| :---: | :---: |
| 需要选择学习率α | 一次运算得出 |
| 当特征数量n大时也能较好适用 | 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为O\(n3\)，通常来说当n小于10000时还是可以接受的 |
| 适用于各种类型的模型 | 只适用于线性模型，不适合逻辑回归模型等其他模型 |



